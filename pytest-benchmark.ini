# pytest-benchmark configuration
# This file configures how benchmarks are run and reported

[tool:pytest-benchmark]
# Only run benchmarks that are marked as benchmarks
only-benchmark = true

# Disable warnings during benchmark runs for cleaner output  
disable-warnings = true

# Minimum number of rounds for each benchmark
min-rounds = 20

# Maximum time to spend on each benchmark (seconds)
max-time = 10.0

# Calibration precision - how precisely to measure timer overhead
calibration-precision = 10

# Sort results by mean execution time
sort = mean

# Columns to display in benchmark results
columns = min, max, mean, stddev, rounds, iterations

# Compare against previous benchmark results
compare = 0001

# Fail benchmarks that regress by more than this percentage
compare-fail = mean:10%

# Auto-save baseline for future comparisons  
autosave = true

# Generate histogram plots of benchmark distributions
histogram = true
